## 一、网络结构

   ![LeNet-5 结构](https://github.com/buluslee/DT-AI/assets/93359778/484457af-d8c8-4fba-a46e-3ee160ac5b2f)


**LeNet-5** 是一个经典的卷积神经网络，它的网络结构如上图所示。整个网络包含了输入层、卷积层、池化层、激活函数、全连接层和输出层。具体结构为：输入层 -> 卷积层 -> 池化层 -> 激活函数 -> 卷积层 -> 池化层 -> 激活函数 -> 卷积层 -> 全连接层 -> 全连接层 -> 输出层。

LeNet-5 网络一共包括了7层（不包括输入层），命名为 C1、S2、C3、S4、C5、F6、OUTPUT。

输入层（INPUT）是 32x32 像素的图像，注意通道数为1。

### 几个重要参数：

- 层编号特点：英文字母 + 数字，其中英文字母代表不同类型的层（C：卷积层，S：池化层，F：全连接层），数字代表第几层。
- 每个卷积核对应一个权重 w 和一个偏置 b。

### 层次解释：

- **C1 层**：第一个卷积层，使用6个大小为5x5的卷积核，padding=0，stride=1 进行卷积，得到6个大小为28x28的特征图：32 - 5 + 1=28。   ![C1 层](https://github.com/buluslee/DT-AI/assets/93359778/07dabd5d-7dc5-43e8-a34e-1d73060a369d)

  ***参数个数***：(5 * 5 + 1) * 6 = 156，其中5 * 5为卷积核的25个参数w，1为偏置项b。
  
  ***连接数***：156 * 28 * 28=122304，其中156为单次卷积过程连线数，28 * 28为输出特征层，每一个像素都由前面卷积得到，即总共经历28 * 28次卷积。 

- **S2 层**：第一个池化层，使用6个大小为2x2的卷积核，padding=0，stride=2 进行池化操作，得到6个大小为14x14的特征图：28 / 2 = 14。   ![S2 层](https://github.com/buluslee/DT-AI/assets/93359778/372d615a-a1c4-4675-b08d-92961cb7e72f)

     S2 层其实相当于降采样层+激活层。先是降采样，然后激活函数 sigmoid 非线性输出。先对 C1 层 2x2 的视野求和，然后进入激活函数，即：

     ![sigmode激活函数](https://github.com/buluslee/DT-AI/assets/93359778/16a27f09-bf7f-42fb-b5f5-2361161747bb)

  ***参数个数***：(1 + 1) * 6=12，其中第一个 1 为池化对应的 2 * 2 感受野中最大的那个数的权重 w，第二个 1 为偏置 b。

  ***连接数***：(2 * 2 + 1) * 6 * 14* 14= 5880，虽然只选取 2 * 2 感受野之和，但也存在 2 * 2 的连接数，1 为偏置项的连接，14 * 14 为输出特征层，每一个像素都由前面卷积得到，即总共经历 14 * 14 次卷积。

- **C3 层**：第二个卷积层，使用16个大小为5x5xn（不同组的输入）的卷积核，padding=0，stride=1 进行卷积操作，得到16个大小为10x10的特征图：14 - 5 + 1 = 10。16 个卷积核并不是都与 S2 的 6 个通道层进行卷积操作，如下图所示，C3 的前六个特征图（0,1,2,3,4,5）由 S2 的相邻三个特征图作为输入，对应的卷积核尺寸为：5x5x3；接下来的 6 个特征图（6,7,8,9,10,11）由 S2 的相邻四个特征图作为输入对应的卷积核尺寸为：5x5x4；接下来的 3 个特征图（12,13,14）号特征图由 S2 间断的四个特征图作为输入对应的卷积核尺寸为：5x5x4；最后的 15 号特征图由 S2 全部(6 个)特征图作为输入，对应的卷积核尺寸为：5x5x6。![C3 层](https://github.com/buluslee/DT-AI/assets/93359778/cdcf931e-ef35-4a9f-8d15-ab3f80adb71a)

  值得注意的是，卷积核是 5×5 且具有 3 个通道，每个通道各不相同，这也是下面计算时 5*5 后面还要乘以3,4,6的原因。这是多通道卷积的计算方法。

     ![image](https://github.com/buluslee/DT-AI/assets/93359778/61480a36-69dd-47ed-a1e3-82b21e1bb33f)

  ***参数个数***：(5 * 5 * 3+1) * 6+(5 * 5 * 4 + 1) * 6 + (5 * 5 * 4 + 1) * 3 + (5 * 5 * 6 + 1) * 1 = 1516。

  ***连接数***：1516 * 10 * 10 = 151600。10 * 10为输出特征层，每一个像素都由前面卷积得到，即总共经历10 * 10次卷积。

- **S4 层**：第二个池化层，使用16个大小为2x2的卷积核，padding=0，stride=2 进行池化操作，得到16个大小为5x5的特征图: 10 / 2 = 5。
  
  ***参数个数***：(1 + 1) * 16 = 32。

  ***连接数***：(2 * 2 + 1) * 16 * 5 * 5 = 2000。

- **C5 层**：第三个卷积层，使用120个大小为5x5x16的卷积核，padding=0，stride=1 进行卷积操作，得到120个大小为1x1的特征图。即相当于 120 个神经元的全连接层。值得注意的是，与C3层不同，这里120个卷积核都与S4的16个通道层进行卷积操作。

  ***参数个数***：(5 * 5 * 16 + 1) * 120 = 48120。

  ***连接数***：48120 * 1 * 1=48120。

- **F6 层**：F6 是全连接层，共有 84 个神经元，与 C5 层进行全连接，即每个神经元都与 C5 层的 120 个特征图相连。计算输入向量和权重向量之间的点积，再加上一个偏置，结果通过 sigmoid 函数输出。

   F6 层有 84 个节点，对应于一个 7x12 的比特图，-1 表示白色，1 表示黑色，这样每个符号的比特图的黑白色就对应于一个编码。该层的训练参数和连接数是(120 + 1)x84=10164。ASCII 编码图如下：

     ![ASCII 编码图](https://github.com/buluslee/DT-AI/assets/93359778/9d9e4892-ba1f-47e3-a4b4-34c00af1d405)
  
  ***参数个数***：(120+1) * 84 = 10164。

  ***连接数***：( 120 + 1) * 84=10164。

- **Output 层**：最后的 Output 层也是全连接层，是 Gaussian Connections，采用了 RBF 函数（即径向欧式距离函数），计算输入向量和参数向量之间的欧式距离（目前已经被Softmax 取代）。Output 层共有 10 个节点，分别代表数字 0 到 9。假设x是上一层的输入，y 是 RBF的输出，则 RBF 输出的计算方式是：

     ![image](https://github.com/buluslee/DT-AI/assets/93359778/0fab8917-ef26-456b-b3ff-f55e3c5e512f)

   上式中 i 取值从 0 到 9，j 取值从 0 到 7*12-1，w 为参数。RBF 输出的值越接近于 0，则越接近于 i，即越接近于 i 的 ASCII 编码图，表示当前网络输入的识别结果是字符 i。下图是数字 3 的识别过程：

     ![image](https://github.com/buluslee/DT-AI/assets/93359778/43368200-eab3-4b23-9344-ce90295c056c)

  ***参数个数***：84 * 10 = 840。

  ***连接数***：84 * 10=840。


### 参数与连接数量：

- C1 层：参数156，连接数122,304。
- S2 层：参数12，连接数5,880。
- C3 层：参数1,516，连接数151,600。
- S4 层：参数32，连接数2,000。
- C5 层：参数48,120，连接数48,120。
- F6 层：参数10,164，连接数10,164。
- Output 层：参数840，连接数840。

## 二、总结

LeNet-5共7层（不含输入层），通过卷积、池化、全连接等层次操作，实现对手写体字符的高效识别。尽管与现代卷积神经网络在细节上存在差异，如激活函数、池化处理和输出层的选择，但LeNet-5为CNN的发展奠定了基础，通过有效地提取图像特征，使得从原始像素中识别规律变得更加容易。

虽然 LeNet-5 在一些细节上与现代卷积神经网络存在差异，如激活函数、池化层处理和输出层的选择，但它为 CNN 的发展奠定了基础。通过有效地提取图像特征，LeNet-5 在早期就实现了对手写字符的高效识别。虽然 LeNet-5 在当时面临数据量不足和计算能力有限的挑战，但它为深度学习的发展铺平了道路。




