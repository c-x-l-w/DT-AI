# AlexNet网络结构

AlexNet是一种经典的深度卷积神经网络，它在计算机视觉领域取得了重大突破。下面将详细介绍AlexNet的网络结构。

## 1. 网络结构

AlexNet整体的网络结构包括：

- 1个输入层（Input layer）
- 5个卷积层（C1、C2、C3、C4、C5）
- 2个全连接层（FC6、FC7）
- 1个输出层（Output layer）

由于AlexNet的结构复杂，参数庞大，为了训练的可行性，采用了两块GTX 580 3GB GPU进行并行训练。这导致卷积层被分成两部分，每部分的FeatureMap分别在两块GPU上进行训练，以减轻硬件资源的负担。

值得一提的是，卷积层C2、C4、C5中的卷积核只与位于同一GPU的上一层的FeatureMap相连，而C3的卷积核与两块GPU的上一层的FeatureMap都连接。

![网络结构](https://github.com/buluslee/DT-AI/assets/93359778/ff189afd-482a-4501-948a-46e1044a2ee1)


### 1.1 输入层（Input layer）

原论文中，AlexNet的输入图像尺寸是224x224x3，但实际图像尺寸为227x227x3。这可能是在写论文时的手误或后续网络调整的结果。

### 1.2 卷积层（C1）

C1层的处理流程包括卷积、ReLU激活、局部响应归一化（LRN）和池化。

- 卷积：输入为227x227x3，使用96个11x11x3的卷积核，padding=0，stride=4，计算得到输出为55x55x96。
- ReLU：对卷积层输出的FeatureMap应用ReLU激活函数。
- 局部响应归一化（LRN）：LRN是一种提高模型准确度的技术方法，通常在激活和池化之后进行。它通过竞争机制增强了响应较大的神经元，并抑制了较小的神经元，从而增强了模型的泛化能力。

### 1.3 卷积层（C2）

C2层的处理流程与C1类似，包括卷积、ReLU激活、LRN和池化。

- 卷积：两组输入各为27x27x48，每组使用128个5x5x48的卷积核，padding=2，stride=1，计算得到每组的输出为27x27x128。
- ReLU：对卷积层输出的FeatureMap应用ReLU激活函数。
- 局部响应归一化（LRN）：使用参数k=2，n=5，α=0.0001，β=0.75进行归一化。
- 池化：使用3x3的池化单元，stride=2进行最大池化操作，得到每组的输出为13x13x128。

### 1.4 卷积层（C3）

C3层包括卷积和ReLU激活。

- 卷积：输入为13x13x256，使用384个3x3x256的卷积核，padding=1，stride=1，计算得到输出为13x13x384。
- ReLU：对卷积层输出的FeatureMap应用ReLU激活函数，将输出分成两组，每组大小为13x13x192，分别位于单个GPU上。

### 1.5 卷积层（C4）

C4层也包括卷积和ReLU激活。

- 卷积：输入为13x13x256，使用384个3x3x256的卷积核，padding=1，stride=1，计算得到输出为13x13x384。
- ReLU：对卷积层输出的FeatureMap应用ReLU激活函数，将输出分成两组，每组大小为13x13x192，分别位于单个GPU上。

### 1.6 卷积层（C5）

C5层的处理流程包括卷积、ReLU激活和池化。

- 卷积：两组输入均为13x13x192，每组使用128个3x3x192的卷积核，padding=1，stride=1，计算得到每组的输出为13x13x128。
- ReLU：对卷积层输出的FeatureMap应用ReLU激活函数。
- 池化：使用3x3的池化单元，stride=2进行最大池化操作，得到每组的输出为6x6x128。

### 1.7 全连接层（FC6）

FC6层的流程包括卷积、ReLU激活、Dropout（卷积）。

- 全连接：输入为6×6×256，使用4096个6×6×256的卷积核进行卷积，输出为1x1x4096。这个层被称为全连接层。
- ReLU：4096个神经元的输出通过ReLU激活函数。
- Dropout：随机断开某些神经元的连接，以防止过拟合。4096个神经元均分到两块GPU上进行计算。

### 1.8 全连接层（FC7）

FC7层的流程也包括卷积、ReLU激活、Dropout。

- 全连接：输入为4096个神经元，输出也是4096个神经元。
- ReLU：4096个神经元的输出通过ReLU激活函数。
- Dropout：

继续随机断开某些神经元的连接，以防止过拟合。

### 1.9 输出层（Output layer）

输出层的流程包括卷积、全连接和Softmax。

- 全连接：输入为4096个神经元，输出为1000个神经元，对应1000个检测类别。
- Softmax：对这1000个神经元的输出应用Softmax函数，得到1000个类别对应的预测概率值。

这就是AlexNet的完整网络结构，它在深度学习和计算机视觉领域取得了巨大成功，并为后续深度学习研究奠定了基础。

# 网络参数

## 2.1 AlexNet神经元数量

整个AlexNet网络包含的神经元个数为：

```
290,400 + 186,624 + 64,896 + 64,896 + 43,264 + 4,096 + 4,096 + 1,000 = 659,272
```

大约65万个神经元。

## 2.2 AlexNet参数数量

整个AlexNet网络包含的参数数量为：

```
34,944 + 307,456 + 885,120 + 663,936 + 442,624 + 37,752,832 + 16,781,312 + 4,096,000 = 60,964,224
```

大约6千万个参数。

设定每个参数是32位浮点数，每个浮点数4个字节。这样参数占用的空间为：

```
60,964,224 x 4 = 243,856,896 字节 = 238,141.5 KB = 232.56 MB
```

参数共占用了大约232MB的空间。

## 2.3 FLOPs

FLOPS（即“每秒浮点运算次数”，“每秒峰值速度”），是“每秒所执行的浮点运算次数”的缩写。它常被用来估算电脑的执行效能，尤其是在使用到大量浮点运算的科学计算领域中。正因为FLOPS字尾的那个S，代表秒，而不是复数，所以不能省略掉。

- 一个MFLOPS（megaFLOPS）等于每秒一百万（=10^6）次的浮点运算。
- 一个GFLOPS（gigaFLOPS）等于每秒十亿（=10^9）次的浮点运算。
- 一个TFLOPS（teraFLOPS）等于每秒一万亿（=10^12）次的浮点运算（1太拉）。
- 一个PFLOPS（petaFLOPS）等于每秒一千万亿（=10^15）次的浮点运算。
- 一个EFLOPS（exaFLOPS）等于每秒一百京（=10^18）次的浮点运算。

在AlexNet网络中，对于卷积层，FLOPS=num_params∗(H∗W)，其中num_params为参数数量，H*W为卷积层的高和宽。对于全连接层，FLOPS=num_params。

# AlexNet创新之处

## 3.1 数据增强（Data Augmentation）

在AlexNet中，作者采用了两种数据增强方法，分别是：

### 3.1.1 镜像反射和随机剪裁

首先，对图像进行镜像反射，然后，在原图和镜像反射的图像（256×256）中随机裁剪出227×227的区域，在测试阶段，对图像的左上、右上、左下、右下、中间分别进行5次裁剪，然后进行翻转，共计10个裁剪，最后对结果进行平均。

### 3.1.2 改变训练样本RGB通道的强度值

另一种数据增强方法是通过对RGB空间进行主成分分析（PCA），然后对主成分进行（0, 0.1）的高斯扰动。这种方法可以改变颜色和光照，结果使错误率降低了1%。

## 3.2 激活函数ReLU

在AlexNet中，作者采用了ReLU（Rectified Linear Unit）作为激活函数，而不是当时标准的tanh()函数。这个非饱和的非线性函数在梯度下降时比tanh()函数快得多。

ReLU函数是一个分段线性函数，当输入小于等于0时输出为0，当输入大于0时输出等于输入。在反向传播中，ReLU的导数对于输出部分始终为1。此外，ReLU会将一部分神经元的输出置为0，从而增加了网络的稀疏性，减少了参数之间的依赖关系，有助于缓解过拟合问题。

下图展示了在一个4层卷积网络中使用ReLU函数在CIFAR-10数据集上训练错误率达到25%的速度比使用tanh函数快6倍。


## 3.3 局部响应归一化（LRN）

局部响应归一化（LRN）通过对局部神经元的活动引入竞争机制，增强了模型的泛化能力。它使响应较大的神经元更大，抑制了响应较小的神经元。

## 3.4 Dropout

Dropout是一种用于抑制过拟合的常用方法，在AlexNet的全连接层FC6和FC7中使用了Dropout。Dropout通过随机将某一层的神经元置为0，从而在训练时减少了神经元之间的依赖关系。这可以看作是一种模型组合，通过组合多个模型来减少过拟合，而只需要两倍的训练时间。

## 3.5 重叠池化

与以前的CNN使用平均池化不同，AlexNet全部使用最大池化。这避免了平均池化可能引入的模糊化效果，并且采用了较小的池化核步长，使池化层的输出之间有重叠，增加了特征的多样性。重叠池化还有助于避免过拟合，贡献了0.3%的Top-5错误率降低。

## 3.6 双GPU训练

考虑到当时的硬件水平，AlexNet采用了双GPU训练的工程创新。双GPU训练不仅减少了训练时间，还降低了top-1和top-5错误率分别1.7%和1.2%。

## 3.7 端到端训练

AlexNet采用了端到端的训练方法，即将原始RGB图像的像素值直接输入到CNN中进行训练，而不像以前那样使用特征提取算法进行预处理。只需将每个像素减去训练集的均值，没有其他图像预处理。这一方法简化了网络的训练流程。
