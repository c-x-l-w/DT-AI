{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 门控循环单元（GRU）详细介绍\n",
    "\n",
    "门控循环单元（Gated Recurrent Unit，GRU）是一种循环神经网络（RNN）的变种，设计用于处理序列数据，并且具有较少的门控单元相比于传统的LSTM。GRU结构简单，但在许多序列建模任务中表现出色。\n",
    "\n",
    "GRU（Gate Recurrent Unit）是循环神经网络（Recurrent Neural Network, RNN）的一种。和LSTM（Long-Short Term Memory）一样，也是为了解决长期记忆和反向传播中的梯度等问题而提出来的。\n",
    "\n",
    "GRU和LSTM在很多情况下实际表现上相差无几，那么为什么我们要使用新人GRU（2014年提出）而不是相对经受了更多考验的LSTM（1997提出）呢\n",
    "\n",
    "## GRU 架构\n",
    "\n",
    "GRU引入了重置门（Reset Gate）和更新门（Update Gate）的概念，以及候选隐藏状态（Candidate Hidden State），从而使其能够更好地捕获长期依赖关系。\n",
    "\n",
    "1. **重置门**：重置门决定了前一个时间步的隐藏状态如何影响当前时间步的输入。它控制着保留和遗忘旧的上下文信息。\n",
    "\n",
    "2. **更新门**：更新门用于调整前一个时间步的隐藏状态在当前时间步的重要性。更新门类似于LSTM中的遗忘门，它决定了前一个时间步的隐藏状态在当前时间步的更新程度。\n",
    "\n",
    "3. **候选隐藏状态**：GRU计算候选隐藏状态，这是一个通过更新门与前一个时间步的隐藏状态相乘得到的值。它代表了前一个时间步的隐藏状态的更新版本。\n",
    "\n",
    "4. **最终隐藏状态**：GRU的最终隐藏状态是由候选隐藏状态和更新门相加得到的。这个最终状态将传递给下一个时间步或用于特定任务。\n",
    "\n",
    "以下是GRU的结构示意图：\n",
    "  输入输出结构图：\n",
    "![GRU Architecture](../img/10.png)\n",
    "  内部结构图\n",
    "![GRU Architecture](../img/11.png)\n",
    "\n",
    "## 重置门控（Reset Gate），\n",
    "我们可以选择性地“重置”之前的隐藏状态，以便在当前时间步考虑更多的新信息。重置门通过以下方式计算：\n",
    "\n",
    "\\[ r_t = \\sigma(W_{r} \\cdot [h_{t-1}, x_t]) \\]\n",
    "\n",
    "其中，\\( r_t \\) 是重置门控信号，\\( h_{t-1} \\) 是前一个时间步的隐藏状态，\\( x_t \\) 是当前时间步的输入，\\( W_{r} \\) 是重置门的权重。\n",
    "![GRU Architecture](../img/12.png)\n",
    "## 应用领域\n",
    "\n",
    "GRU在自然语言处理（NLP）等领域广泛应用，适用于以下任务：\n",
    "\n",
    "- **语言建模**：GRU能够捕获文本中的长期依赖关系，适用于语言建模任务。\n",
    "- **机器翻译**：GRU在序列到序列的任务中表现出色，如机器翻译，其中需要理解输入句子的上下文信息。\n",
    "\n",
    "## 与 LSTM 的比较\n",
    "\n",
    "与LSTM相比，GRU具有较少的门控单元，因此具有更少的参数和更快的训练速度。在某些任务中，GRU甚至表现得比LSTM更好。\n",
    "\n",
    "## 总结\n",
    "\n",
    "门控循环单元（GRU）是一种强大的循环神经网络变体，适用于序列数据处理。它通过引入重置门和更新门，以及候选隐藏状态，能够更好地捕获序列中的长期依赖关系。GRU在自然语言处理和其他序列任务中都发挥着重要作用。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 训练和测试数据：\n",
    "\n",
    "链接：https://pan.baidu.com/s/1K9Eju4sNsrde8th56QPlVA\n",
    "提取码：cvof"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import csv\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch.nn.utils.rnn import pack_padded_sequence # https://pytorch.org/docs/master/generated/torch.nn.utils.rnn.pack_padded_sequence.html\n",
    "import math\n",
    "\n",
    "# 1：数据集\n",
    "\n",
    "# 超参数\n",
    "HIDDEN_SIZE = 100 # 隐藏层\n",
    "BATCH_SIZE = 256\n",
    "N_LAYER = 2 # RNN的层数\n",
    "N_EPOCHS = 100 # train的轮数\n",
    "N_CHARS = 128 # 这个就是要构造的字典的长度\n",
    "USE_GPU = False\n",
    "\n",
    "class NameDataset(Dataset):  # 这个是自己写的数据集的类，就那3个函数\n",
    "    def __init__(self, is_train_set=True):\n",
    "        filename = \"E:\\\\PyTorch\\\\PyTorch深度学习实践\\\\names_train.csv\" if is_train_set else \"E:\\\\PyTorch\\\\PyTorch深度学习实践\\\\names_test.csv\"\n",
    "        with open(filename, \"rt\") as f:  # 因为这个文件不是很大，所以在初始化的时候就全读进来了\n",
    "            reader = csv.reader(f)\n",
    "            rows = list(reader)\n",
    "        self.names = [row[0] for row in rows]\n",
    "        self.len = len(self.names)\n",
    "        self.countries = [row[1] for row in rows]\n",
    "        self.country_list = list(sorted(set(self.countries)))  # 去重+排序\n",
    "        self.country_dict = self.getCountryDict()  # 做一个国家词典,这个就是标签 y\n",
    "        self.country_num = len(self.country_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.names[index], self.country_dict[self.countries[index]] # 前者是名字字符串，后者是国家的索引\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def getCountryDict(self):\n",
    "        country_dict = dict()\n",
    "        for idx, country_name in enumerate(self.country_list, 0):\n",
    "            country_dict[country_name] = idx\n",
    "        return country_dict\n",
    "\n",
    "    def idx2country(self, index): # 这个就是为了得到分类之后，返回下标对应的字符串，也就是显示使用的\n",
    "        return self.country_list[index]\n",
    "\n",
    "    def getCountriesNum(self):  # 分类的国家数量\n",
    "        return self.country_num\n",
    "\n",
    "def make_tensors(names, countries): # 这个就是将名字的字符串转换成数字表示\n",
    "    sequences_and_lengths = [name2list(name)for name in names]  # [(),(),,...]\n",
    "    name_sequences = [sl[0] for sl in sequences_and_lengths] # 取转换成ACCIIS的序列,长度是BatchSize\n",
    "    seq_lengths = torch.LongTensor([sl[1]for sl in sequences_and_lengths]) # 取序列的长度，转换成longtensor\n",
    "    countries = countries.long() #这个cluntries之前转换成了数字，这里只转换成longtensor\n",
    "\n",
    "    # make tensor of name, BatchSize x SeqLen\n",
    "    seq_tensor = torch.zeros(len(name_sequences), seq_lengths.max()).long() #先做全0的张量，然后填充,长度是BatchSize\n",
    "    for idx, (seq, seq_len) in enumerate(zip(name_sequences, seq_lengths), 0):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "\n",
    "    # sort by length to use pack_padded_sequence\n",
    "    seq_lengths, perm_idx = seq_lengths.sort(dim=0, descending=True)\n",
    "    seq_tensor = seq_tensor[perm_idx]\n",
    "    countries = countries[perm_idx]\n",
    "\n",
    "    return create_tensor(seq_tensor), \\\n",
    "           create_tensor(seq_lengths), \\\n",
    "           create_tensor(countries)\n",
    "\n",
    "def name2list(name): # 将name字符串的字母转换成ASCII\n",
    "    arr = [ord(c) for c in name]\n",
    "    return arr, len(arr)  # 返回的是元组\n",
    "\n",
    "def create_tensor(tensor):  # 是否使用GPU\n",
    "    if USE_GPU:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        tensor = tensor.to(device)\n",
    "    return tensor\n",
    "\n",
    "trainset = NameDataset(is_train_set=True) # train数据\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "testset = NameDataset(is_train_set=False) # test数据\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "N_COUNTRY = trainset.getCountriesNum() # 这个就是总的类别的数量"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 2：构造模型\n",
    "class RNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    这里的bidirectional就是GRU是不是双向的，双向的意思就是既考虑过去的影响，也考虑未来的影响（如一个句子）\n",
    "    具体而言：正向hf_n=w[hf_{n-1}, x_n]^T,反向hb_0,最后的h_n=[hb_0, hf_n],方括号里的逗号表示concat。\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_directions = 2 if bidirectional else 1 # 双向2、单向1\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,  # 输入维度、输出维度、层数、bidirectional用来说明是单向还是双向\n",
    "                         bidirectional=bidirectional)\n",
    "        self.fc = nn.Linear(hidden_size * self.n_directions, output_size)\n",
    "\n",
    "    def __init__hidden(self, batch_size):  # 工具函数，作用是创建初始的隐藏层h0\n",
    "        hidden = torch.zeros(self.n_layers * self.n_directions,\n",
    "                            batch_size, self.hidden_size)\n",
    "        return create_tensor(hidden) # 加载GPU\n",
    "\n",
    "    def forward(self, input, seq_lengths):\n",
    "        # input shape:B * S -> S * B\n",
    "        input = input.t()\n",
    "        batch_size = input.size(1)\n",
    "\n",
    "        hidden = self.__init__hidden(batch_size) # 隐藏层h0\n",
    "        embedding = self.embedding(input)\n",
    "\n",
    "        # pack them up\n",
    "        gru_input = pack_padded_sequence(embedding, seq_lengths)  # 填充了可能有很多的0，所以为了提速，将每个序列以及序列的长度给出\n",
    "\n",
    "        output, hidden = self.gru(gru_input, hidden) # 只需要hidden\n",
    "        if self.n_directions == 2: #双向的，则需要拼接起来\n",
    "            hidden_cat = torch.cat([hidden[-1], hidden[-2]], dim=1)\n",
    "        else:\n",
    "            hidden_cat = hidden[-1] # 单向的，则不用处理\n",
    "        fc_output = self.fc(hidden_cat) # 最后来个全连接层,确保层想要的维度（类别数）\n",
    "        return fc_output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# 4：训练和测试模型\n",
    "def trainModel():\n",
    "    total_loss = 0\n",
    "    for i, (names, countries) in enumerate(trainloader, 1): # 记载的下标从1开始\n",
    "        inputs, seq_lengths, target = make_tensors(names, countries)\n",
    "        output = classifier(inputs, seq_lengths) # 预测输出\n",
    "        loss = criterion(output, target) # 求出损失\n",
    "        optimizer.zero_grad() # 清除之前的梯度\n",
    "        loss.backward() # 梯度反传\n",
    "        optimizer.step() # 更新参数\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if i % 10 ==0:\n",
    "            print(f'[{time_since(start)}] Epoch {epoch}', end='')\n",
    "            print(f'[{i * len(inputs)}/{len(trainset)}]', end='')\n",
    "            print(f'loss={total_loss / (i * len(inputs))}')\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def testModel():\n",
    "    correct = 0\n",
    "    total = len(testset)\n",
    "    print(\"evaluating trained model...\")\n",
    "    with torch.no_grad():\n",
    "        for i, (names, countries) in enumerate(testloader, 1):\n",
    "            inputs, seq_lengths, target = make_tensors(names, countries) # 将名字的字符串转换成数字表示\n",
    "            output = classifier(inputs, seq_lengths) # 预测输出\n",
    "            pred = output.max(dim=1, keepdim=True)[1] # 预测出来是个向量，里面的值相当于概率，取最大的\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() # 预测和实际标签相同则正确率加1\n",
    "\n",
    "        percent = '%.2f' % (100 * correct / total)\n",
    "        print(f'Test set:Accuracy{correct} / {total} {percent}%')\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRY, N_LAYER) # 定义模型\n",
    "    if USE_GPU:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        classifier.to(device)\n",
    "\n",
    "    # 第三步：定义损失函数和优化器\n",
    "    criterion = torch.nn.CrossEntropyLoss() # 分类问题使用交叉熵损失函数\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001) # 使用了随机梯度下降法\n",
    "\n",
    "    start = time.time()\n",
    "    print(\"Training for %d epochs...\" % N_EPOCHS)\n",
    "    acc_list = []\n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        # Train cycle\n",
    "        trainModel()\n",
    "        acc = testModel()\n",
    "        acc_list.append(acc) # 存入列表，后面画图使用\n",
    "\n",
    "    # 画图\n",
    "    epoch = np.arange(1, len(acc_list) + 1, 1) # 步长为1\n",
    "    acc_list = np.array(acc_list)\n",
    "    plt.plot(epoch, acc_list)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid() # 显示网格线 1=True=默认显示；0=False=不显示\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}