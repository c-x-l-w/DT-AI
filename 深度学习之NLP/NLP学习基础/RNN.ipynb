{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环神经网络（RNN）介绍\n",
    "\n",
    "## 什么是循环神经网络？\n",
    "\n",
    "循环神经网络（Recurrent Neural Network，RNN）是一种深度学习模型，专门设计用于处理序列数据，例如时间序列、自然语言、音频信号等。相较于传统的前馈神经网络，RNN 具备一种独特的结构，使得网络可以在处理序列数据时在不同时间步之间共享信息，从而更好地捕捉序列中的时间相关性和模式。\n",
    "\n",
    "\n",
    "## RNN 的结构和工作原理\n",
    "\n",
    "RNN 的核心思想是引入循环结构，使得网络在每个时间步都可以接收当前时间步的输入和上一个时间步的隐藏状态。这使得网络具有了一种记忆能力，可以将之前时间步的信息传递到后续时间步中。\n",
    "\n",
    "RNN 的一个基本单元如下图所示：\n",
    "\n",
    "![RNN Cell](../img/1.png)\n",
    "\n",
    "在上图中，`xt` 表示输入序列在时间步 t 的输入，`at` 表示在时间步 t 的隐藏状态，`Yt` 表示在时间步 t 的输出。RNN 在每个时间步都执行相同的操作，输入数据和上一个时间步的隐藏状态通过权重矩阵进行组合，并经过激活函数（通常为 tanh 或 sigmoid）得到新的隐藏状态和输出。\n",
    "\n",
    "首先看一个简单的循环神经网络如，它由输入层、一个隐藏层和一个输出层组成：\n",
    "![RNN Cell](../img/2.png)\n",
    "当我们刚开始学习时，这种图可能会感到**抽象和困惑**。我在刚开始学习的时候也曾有过类似的疑问，诸如每个节点到底代表输入的一个值，还是一整层的**向量集合**？另外，隐藏层如何连接到自身？等等问题。\n",
    "\n",
    "让我们用更直观的方式理解这个图。如果我们将上面带有 \"W\" 的箭头圈去掉，它就变成了最基本的**全连接神经网络**。在这里：\n",
    "\n",
    "- **\"x\" 是一个向量**，代表输入层的值（虽然图中没有显示出代表神经元节点的圆圈）；\n",
    "- **\"s\" 也是一个向量**，代表隐藏层的值（虽然图中隐藏层只画了一个节点，你可以想象这一层实际上有多个节点，节点数量与向量 \"s\" 的维度相同）。\n",
    "\n",
    "**\"U\" 表示输入层到隐藏层的权重矩阵**，**\"o\" 是一个向量**，代表输出层的值；**\"V\" 表示隐藏层到输出层的权重矩阵**。\n",
    "\n",
    "接下来，我们来看一下 **\"W\" 是什么**。循环神经网络中，隐藏层的值 \"s\" 不仅取决于当前时刻的输入 \"x\"，还取决于上一时刻隐藏层的值 \"s\"。而**权重矩阵 \"W\" 正是用来将上一时刻的隐藏层值作为当前时刻的输入进行加权的**。\n",
    "<img src=\"../img/1.jpg\" alt=\"RNN Cell\" width=\"1200\"/>\n",
    "\n",
    "在循环神经网络中，我们可以用以下公式来表示其计算方法：\n",
    "\n",
    "在时间步（时刻）**t**：\n",
    "\n",
    "- 输入层的值为 **x<sub>t</sub>**。\n",
    "- 隐藏层的值为 **s<sub>t</sub>**，该值不仅仅取决于当前时刻的输入 **x<sub>t</sub>**，还取决于上一时刻隐藏层的值 **s<sub>t-1</sub>**。计算方式为：\n",
    "\n",
    "  **s<sub>t</sub> = tanh(U * x<sub>t</sub> + W * s<sub>t-1</sub>)**\n",
    "\n",
    "  其中，**U** 表示输入层到隐藏层的权重矩阵，**W** 是隐藏层到自身的权重矩阵，**tanh** 是双曲正切函数，用于增强网络的非线性能力。\n",
    "\n",
    "- 输出层的值为 **o<sub>t</sub>**，其计算方式与常规神经网络一样：\n",
    "\n",
    "  **o<sub>t</sub> = softmax(V * s<sub>t</sub>)**\n",
    "\n",
    "  其中，**V** 表示隐藏层到输出层的权重矩阵，**softmax** 是用于将输出转化为概率分布的函数，使得各个类别的输出概率之和为1。\n",
    "\n",
    "这样，循环神经网络在每个时间步都根据当前输入和前一时刻的隐藏状态来计算隐藏状态和输出值，从而在序列数据上实现记忆和学习。\n",
    "<img src=\"../img/3.png\" alt=\"RNN Cell\" width=\"800\"/>\n",
    "\n",
    "然而，传统的 RNN 存在**梯度消失**和**梯度爆炸**等问题，导致在处理长序列时难以有效地传递信息。为了解决这个问题，产生了一些改进的 RNN 变体，如长短时记忆网络（LSTM）和门控循环单元（GRU），它们引入了门控机制，更好地捕捉序列中的长距离依赖。\n",
    "\n",
    "## 应用领域\n",
    "\n",
    "RNN 在许多领域都有广泛的应用：\n",
    "\n",
    "- **自然语言处理（NLP）**：RNN 可以用于文本生成、语言建模、机器翻译、文本分类和情感分析等任务。\n",
    "- **语音识别**：RNN 可以处理音频数据，用于语音识别、语音合成等。\n",
    "- **时间序列预测**：RNN 被应用于股票价格预测、天气预测、交通流量预测等。\n",
    "- **图像描述生成**：结合卷积神经网络（CNN），RNN 可以生成图像描述。\n",
    "- **视频分析**：RNN 可用于动作识别、视频标注等。\n",
    "\n",
    "## 实现工具与库\n",
    "\n",
    "许多深度学习框架提供了对 RNN 及其变体的支持，包括 TensorFlow、PyTorch、Keras 等。这些框架提供了高级的接口，帮助您构建、训练和部署 RNN 模型。使用 GPU 可以加速训练过程。\n",
    "\n",
    "## 总结\n",
    "\n",
    "循环神经网络是一种强大的神经网络模型，专门用于处理序列数据。通过引入循环结构和记忆机制，RNN 可以在序列数据中捕捉时间相关性，使其在多个应用领域都取得了卓越的成果。尽管传统的 RNN 存在一些问题，但通过改进的变体如 LSTM 和 GRU，我们能够更好地解决长序列的依赖关系。\n",
    "\n",
    "如需进一步了解如何使用 RNN，您可以查看相关的教程和实际代码示例。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 人名分类示例\n",
    "\n",
    "以下示例展示了如何使用循环神经网络（RNN）实现人名的分类任务。该任务的目标是根据输入的任意长度的姓名（字符串），预测姓名来自哪个国家（18 个类别）。\n",
    "\n",
    "### 数据预处理\n",
    "\n",
    "数据来源：[下载链接](http://download.pytorch.org/tutorial/data.zip)\n",
    "http://download.pytorch.org/tutorial/data.zip\n",
    "对于每个字符，我们首先将其转换为 one-hot 向量，表示为 [0,0,...,1,...,0] 的形式。然后，我们使用 RNN 进行处理。在迭代训练过程中，我们采取以下步骤：\n",
    "\n",
    "1. 随机选择一个标签（国家类别）和一个姓名。\n",
    "2. 将姓名转换为形状为 [length, 1, 57] 的 one-hot 张量，其中 length 表示姓名长度。将标签也转换为形状为 [1] 的张量。\n",
    "3. 初始化隐藏层状态信息。\n",
    "4. 循环遍历姓名中的每个字符的 one-hot 向量，将其输入到 RNN 中。\n",
    "5. 得到输出，即预测的 18 个分类。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import unicodedata\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data\n",
    "from common_tools import set_seed\n",
    "import enviroments\n",
    "\n",
    "set_seed(1)  # 设置随机种子\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# 读取文件并按行分割\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters)\n",
    "\n",
    "# 从 all_letters 中找到字母的索引，例如 \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# 仅用于演示，将字母转换为 <1 x n_letters> 张量\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# 将一行转换为 <line_length x 1 x n_letters>，\n",
    "# 或者是一系列的 one-hot 字母向量\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)                 # 随机选取类别\n",
    "    line = randomChoice(category_lines[category])           # 随机选取一个样本\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)    # 字符串转为 one-hot\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "# 仅返回给定行的输出\n",
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    return output\n",
    "\n",
    "def predict(input_line, n_predictions=3):\n",
    "    print('\\n> %s' % input_line)\n",
    "    with torch.no_grad():\n",
    "        output = evaluate(lineToTensor(input_line))\n",
    "\n",
    "        # 获取前 N 个预测的类别\n",
    "        topv, topi = output.topk(n_predictions, 1, True)\n",
    "\n",
    "        for i in range(n_predictions):\n",
    "            value = topv[0][i].item()\n",
    "            category_index = topi[0][i].item()\n",
    "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "\n",
    "def get_lr(iter, learning_rate):\n",
    "    lr_iter = learning_rate if iter < n_iters else learning_rate*0.1\n",
    "    return lr_iter\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.u = nn.Linear(input_size, hidden_size)\n",
    "        self.w = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "\n",
    "        u_x = self.u(inputs)\n",
    "\n",
    "        hidden = self.w(hidden)\n",
    "        hidden = self.tanh(hidden + u_x)\n",
    "\n",
    "        output = self.softmax(self.v(hidden))\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    line_tensor = line_tensor.to(device)\n",
    "    hidden = hidden.to(device)\n",
    "    category_tensor = category_tensor.to(device)\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # 根据学习率将参数的梯度添加到其值上\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 配置\n",
    "    path_txt = os.path.join(enviroments.names,\"*.txt\")\n",
    "    all_letters = string.ascii_letters + \" .,;'\"\n",
    "    n_letters = len(all_letters)    # 52 + 5 字符总数\n",
    "    print_every = 5000\n",
    "    plot_every = 5000\n",
    "    learning_rate = 0.005\n",
    "    n_iters = 200000\n",
    "\n",
    "    # 步骤 1 数据\n",
    "    # 构建 category_lines 字典，每种语言对应一组名字\n",
    "    category_lines = {}\n",
    "    all_categories = []\n",
    "    for filename in glob.glob(path_txt):\n",
    "        category = os.path.splitext(os.path.basename(filename))[0]\n",
    "        all_categories.append(category)\n",
    "        lines = readLines(filename)\n",
    "        category_lines[category] = lines\n",
    "\n",
    "    n_categories = len(all_categories)\n",
    "\n",
    "    # 步骤 2 模型\n",
    "    n_hidden = 128\n",
    "    rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "\n",
    "    rnn.to(device)\n",
    "\n",
    "    # 步骤 3 损失\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    # 步骤 4 手动优化\n",
    "\n",
    "    # 步骤 5 迭代\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "    start = time.time()\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        # 随机采样\n",
    "        category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "\n",
    "        # 训练\n",
    "        output, loss = train(category_tensor, line_tensor)\n",
    "\n",
    "        current_loss += loss\n",
    "\n",
    "        # 打印迭代次数、损失、名称和预测\n",
    "        if iter % print_every == 0:\n",
    "            guess, guess_i = categoryFromOutput(output)\n",
    "            correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "            print('迭代: {:<7} 时间: {:>8s} 损失: {:.4f} 名称: {:>10s}  预测: {:>8s} 标签: {:>8s}'.format(\n",
    "                iter, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "        # 将当前损失平均值添加到损失列表\n",
    "        if iter % plot_every == 0:\n",
    "            all_losses.append(current_loss / plot_every)\n",
    "            current_loss = 0\n",
    "    path_model = os.path.join(BASE_DIR, \"rnn_state_dict.pkl\")\n",
    "    torch.save(rnn.state_dict(), path_model)\n",
    "    plt.plot(all_losses)\n",
    "    plt.show()\n",
    "\n",
    "    predict('Yue Tingsong')\n",
    "    predict('Yue tingsong')\n",
    "    predict('yutingsong')\n",
    "\n",
    "    predict('test your name')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}